{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dacon.io/competitions/official/235591/codeshare/1110?page=1&dtype=recent&ptype=pub  \n",
    "goldbar님 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Add, BatchNormalization, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "train_files = os.listdir('inputs/train')\n",
    "train = []\n",
    "for file in train_files:\n",
    "    try:\n",
    "        inputs = np.load('inputs/train/'+file).astype('float32')\n",
    "        train.append(inputs)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "test = []\n",
    "for sub_id in submission['id']:\n",
    "    inputs = np.load('inputs/test/'+'subset_'+sub_id+'.npy').astype('float32')\n",
    "    test.append(inputs)\n",
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~10번 채널만 학습에 사용\n",
    "\n",
    "x_train = train[:,:,:,:10]\n",
    "y_train = train[:,:,:,14]\n",
    "test = test[:,:,:,:10]\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가채점용 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.025, random_state=7777)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0보다 작은 값이 포함 된 데이터 삭제\n",
    "y_train_ = y_train.reshape(-1,y_train.shape[1]*y_train.shape[2])\n",
    "\n",
    "x_train = np.delete(x_train, np.where(y_train_<0)[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(y_train_<0)[0], axis=0)\n",
    "\n",
    "y_train = y_train.reshape(-1, x_train.shape[1], x_train.shape[2],1)\n",
    "y_test = y_test.reshape(-1, y_test.shape[1], y_test.shape[2],1)\n",
    "\n",
    "y_train_ = np.delete(y_train_, np.where(y_train_<0)[0], axis=0)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비가 내린 셀이 50개 이상인 데이터만 선택\n",
    "x_train = x_train[np.sum(y_train_, -1) >= 50]\n",
    "y_train = y_train[np.sum(y_train_, -1) >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 회전, 반전시켜 학습 데이터양을 8배로 부풀림\n",
    "\n",
    "def data_generator(x_train, y_train):\n",
    "    rotate_X_90 = np.zeros_like(x_train)\n",
    "    rotate_Y_90 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_90.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "\n",
    "        rotate_X_90[j,:,:,:] = rotate_x\n",
    "        rotate_Y_90[j,:,:,:] = rotate_y\n",
    "\n",
    "    rotate_X_180 = np.zeros_like(x_train)\n",
    "    rotate_Y_180 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_180.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "\n",
    "        rotate_X_180[j,:,:,:] = rotate_x\n",
    "        rotate_Y_180[j,:,:,:] = rotate_y\n",
    "\n",
    "    rotate_X_270 = np.zeros_like(x_train)\n",
    "    rotate_Y_270 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_270.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "\n",
    "        rotate_X_270[j,:,:,:] = rotate_x\n",
    "        rotate_Y_270[j,:,:,:] = rotate_y\n",
    "\n",
    "    x_train = np.concatenate((x_train, rotate_X_90, rotate_X_180, rotate_X_270), axis = 0)\n",
    "    y_train = np.concatenate((y_train, rotate_Y_90, rotate_Y_180, rotate_Y_270), axis = 0)\n",
    "    del rotate_X_90, rotate_X_180, rotate_X_270\n",
    "\n",
    "    x_T = np.zeros_like(x_train)\n",
    "    y_T = np.zeros_like(y_train)\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "        for j in range(x_train.shape[3]):\n",
    "            x_T[i,:,:,j] = x_train[i,:,:,j].T\n",
    "        y_T[i,:,:,0] = y_train[i,:,:,0].T\n",
    "\n",
    "    x_train = np.concatenate((x_train, x_T), axis = 0)\n",
    "    y_train = np.concatenate((y_train, y_T), axis = 0)\n",
    "\n",
    "    del x_T,y_T\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_over_fscore(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: sample_submission.csv 형태의 실제 값\n",
    "    y_pred: sample_submission.csv 형태의 예측 값\n",
    "    '''\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(1, -1)[0]  \n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
    "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
    "    \n",
    "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
    "    IsNotMissing = y_true >= 0\n",
    "    \n",
    "    # mae 계산\n",
    "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
    "    \n",
    "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
    "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    # f1_score 계산    \n",
    "    f_score = f1_score(y_true, y_pred) \n",
    "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
    "    return mae / (f_score + 1e-07) \n",
    "\n",
    "\n",
    "def mae(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs=Input(x_train.shape[1:])\n",
    "    \n",
    "    bn=BatchNormalization()(inputs)\n",
    "    conv0=Conv2D(256, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "    \n",
    "    bn=BatchNormalization()(conv0)\n",
    "    conv=Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat=concatenate([conv0, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    conv=Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat=concatenate([concat, conv], axis=3)\n",
    "        \n",
    "    for i in range(5):\n",
    "        bn=BatchNormalization()(concat)\n",
    "        conv=Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "        concat=concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    outputs=Conv2D(1, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "    \n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강수량을 예측할 지점에서 멀리 있는 데이터일 수록 영향이 적어져 풀링레이어가 결과 예측에 방해가 될 것으로 판단\n",
    "# convolution과 residual block만으로 모델 설계\n",
    "def train_model(x_data, y_data, k, s):\n",
    "    \n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=7777)\n",
    "    \n",
    "    model_number = 0\n",
    "    for train_idx, val_idx in k_fold.split(x_data):\n",
    "        if model_number == s:\n",
    "            x_train, y_train = x_data[train_idx], y_data[train_idx]\n",
    "            x_val, y_val = x_data[val_idx], y_data[val_idx]\n",
    "\n",
    "            # 데이터를 부풀릴시 많은 양의 메모리가 필요\n",
    "            x_train, y_train = data_generator(x_train, y_train)\n",
    "\n",
    "            model = create_model()\n",
    "            print(model.summary())\n",
    "\n",
    "            model.compile(loss='mae', optimizer='adam', metrics=[score, fscore_keras])\n",
    "\n",
    "            callbacks_list = [\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    patience=3,\n",
    "                    factor=0.8\n",
    "                ),\n",
    "\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath = 'models/model'+str(model_number)+'.h5',\n",
    "                    monitor='val_score',\n",
    "                    save_best_only=True\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            model.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_val, y_val), callbacks=callbacks_list)\n",
    "        \n",
    "        model_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "models = []\n",
    "for n in range(k):\n",
    "    train_model(x_train, y_train, k=k, s=n)\n",
    "    model = load_model('models/model'+str(n)+'.h5', custom_objects = {'score':score,'fscore_keras':fscore_keras})\n",
    "    models.append(model)\n",
    "# k = 5\n",
    "# models = []\n",
    "\n",
    "# train_model(x_train, y_train, k=k, s=4)\n",
    "\n",
    "# for n in range(k):\n",
    "#     model = load_model('models/model'+str(n)+'.h5', custom_objects = {'score':score,'fscore_keras':fscore_keras})\n",
    "#     models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(x_test))\n",
    "    print(mae_over_fscore(y_test, preds[-1]))\n",
    "\n",
    "pred = sum(preds)/len(preds)\n",
    "print(mae_over_fscore(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(test))\n",
    "\n",
    "pred = sum(preds)/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1,1600)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('resnet_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
