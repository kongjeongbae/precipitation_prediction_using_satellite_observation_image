{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 및 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from custom_metric import *\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 약 20초 걸림\n",
    "with open('D:/inputs/24/train50.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "train = train[:, :, :, :10]\n",
    "\n",
    "with open('D:/inputs/24/train_y50.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "    train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "with open('D:/inputs/24/test.pickle', 'rb') as f:\n",
    "    TEST = pickle.load(f)\n",
    "TEST = TEST[:, :, :, :10]    \n",
    "\n",
    "\n",
    "with open('inputs/val_index.pickle', 'rb') as f:\n",
    "    val_index = pickle.load(f)\n",
    "\n",
    "VAL_X = train[val_index].copy()\n",
    "VAL_Y = train_y[val_index].copy()\n",
    "\n",
    "\n",
    "# 아웃라이어 수정\n",
    "# train[1,16, 24, -2] = 163.05731201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inputs/train_50up.pickle', 'rb') as f:\n",
    "    up50 = pickle.load(f)\n",
    "up50 = up50[:, :, :, :10]\n",
    "with open('inputs/train_y_50up.pickle', 'rb') as f:\n",
    "    up50_y = pickle.load(f)\n",
    "    up50_y = up50_y.reshape(up50_y.shape[0], 40, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inputs/train_100up.pickle', 'rb') as f:\n",
    "    up100 = pickle.load(f)\n",
    "up100 = up100[:, :, :, :10]\n",
    "with open('inputs/train_y_100up.pickle', 'rb') as f:\n",
    "    up100_y = pickle.load(f)\n",
    "    up100_y = up100_y.reshape(up100_y.shape[0], 40, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inputs/train_150up.pickle', 'rb') as f:\n",
    "    up150 = pickle.load(f)\n",
    "up150 = up150[:, :, :, :10]\n",
    "with open('inputs/train_y_150up.pickle', 'rb') as f:\n",
    "    up150_y = pickle.load(f)\n",
    "    up150_y = up150_y.reshape(up150_y.shape[0], 40, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inputs/train_200up.pickle', 'rb') as f:\n",
    "    up200 = pickle.load(f)\n",
    "up200 = up200[:, :, :, :10]\n",
    "with open('inputs/train_y_200up.pickle', 'rb') as f:\n",
    "    up200_y = pickle.load(f)\n",
    "    up200_y = up200_y.reshape(up200_y.shape[0], 40, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(up50.shape)\n",
    "print(up100.shape)\n",
    "print(up150.shape)\n",
    "print(up200.shape)\n",
    "\n",
    "train = np.vstack([train, up50])\n",
    "\n",
    "train = np.vstack([train, up100])\n",
    "\n",
    "# train = np.vstack([train, up150])\n",
    "\n",
    "\n",
    "# train = np.vstack([train, up200])\n",
    "\n",
    "\n",
    "del up50, up100, up150, up200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.vstack([train_y, up50_y])\n",
    "\n",
    "train_y = np.vstack([train_y, up100_y])\n",
    "\n",
    "# train_y = np.vstack([train_y, up150_y])\n",
    "\n",
    "# train_y = np.vstack([train_y, up200_y])\n",
    "\n",
    "\n",
    "del up50_y, up100_y, up150_y, up200_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target 값 log 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.log(train_y+1)\n",
    "VAL_Y = np.log(VAL_Y+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회전변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 회전변환시 마이너스(-) 하는 경우, 제곱을 해주면 target값과의 상관관계가 높아짐. 테스트해볼것!\n",
    "# v1_m_h1 = ((train[:, :, :, 0] * np.cos(np.pi / 4) - train[:, :, :, 1] * np.sin(np.pi / 4)) ** 0.5) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rotation(train)\n",
    "TEST = rotation(TEST)\n",
    "VAL_X = rotation(VAL_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    data9 = data[:, :, :, 9].copy()\n",
    "    data9 = data9.reshape(data9.shape[0], data9.shape[1], data9.shape[2], 1)\n",
    "    data = np.concatenate([data, data9.copy()], -1)\n",
    "    data = np.concatenate([data, data9.copy()], -1)\n",
    "    data = np.concatenate([data, data9.copy()], -1)\n",
    "    data[:,:,:,10] = data[:,:,:,10] // 100\n",
    "    data[:,:,:,11] = data[:,:,:,11] // 100\n",
    "    data[:,:,:,12] = data[:,:,:,12] // 100\n",
    "    data[:,:,:,10] = np.where(data[:,:,:,10] != 1, 0, 1)\n",
    "    data[:,:,:,11] = np.where(data[:,:,:,11] != 2, 0, 1)\n",
    "    data[:,:,:,12] = np.where(data[:,:,:,12] != 3, 0, 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = one_hot_encoding(train)\n",
    "TEST = one_hot_encoding(TEST)\n",
    "VAL_X = one_hot_encoding(VAL_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape # 01, 23, 4, 56, 78, 9, 10, 11, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:, :, :, 9] = train[:, :, :, 9] / 322\n",
    "TEST[:, :, :, 9] = TEST[:, :, :, 9] / 322\n",
    "VAL_X[:, :, :, 9] = VAL_X[:, :, :, 9] / 322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 경도, 위도 정보 삭제함.\n",
    "# train = train[:, :, :, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 경도, 위도 정보 삭제함.\n",
    "# TEST = TEST[:, :, :, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate([train, train_y], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train1 = np.rot90(train, 1, (1,2))\n",
    "train2 = np.rot90(train, 2, (1,2))\n",
    "train3 = np.rot90(train, 3, (1,2))\n",
    "train_lr = np.fliplr(train)\n",
    "# train_ud = np.flipud(train)\n",
    "\n",
    "train = np.vstack([train, train1])\n",
    "del train1\n",
    "\n",
    "train = np.vstack([train, train2])\n",
    "del train2\n",
    "\n",
    "train = np.vstack([train, train3])\n",
    "del train3\n",
    "\n",
    "train = np.vstack([train, train_lr])\n",
    "del train_lr\n",
    "\n",
    "# train = np.vstack([train, train_ud])\n",
    "# del train_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[:, :, :, -1].copy()\n",
    "train_y = train_y.reshape(train_y.shape[0], train_y.shape[1], train_y.shape[2], 1)\n",
    "train = train[:,:,:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_np로 결합 후 scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ss 약 5분 걸림\n",
    "# from preprocessing import scaling\n",
    "\n",
    "# train, test = scaling(train, test, 'ss') # 3번째 인자 'rs', 'ms', 'ss' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split(train, train_y, test_size=0.025, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언 - Unet, Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_model, build_model2\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Add, BatchNormalization, concatenate, add, MaxPooling2D, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input((40, 40, train.shape[3]))\n",
    "output_layer = build_model(input_layer, 32)\n",
    "\n",
    "# sgd = tf.keras.optimizers.SGD(0.01)\n",
    "# stocastic_avg_opt = tfa.optimizers.SWA(sgd)\n",
    "\n",
    "# ad = tf.keras.optimizers.Adam(0.01)\n",
    "# stocastic_avg_opt = tfa.optimizers.SWA(ad)\n",
    "# model.compile(loss=\"mae\", optimizer=stocastic_avg_opt, metrics=[\"mae\"])\n",
    "#\n",
    "\n",
    "\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras])\n",
    "\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(train.shape[1:])\n",
    "output_layer = build_model(input_layer, 32)\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def deep_cnn_advanced_nin():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(input_shape = (train.shape[1:]), filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    # 1x1 convolution\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    # 1x1 convolution\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    # 1x1 convolution\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    # 1x1 convolution\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (3,3), strides = (1,1), padding = 'same', kernel_initializer='he_normal'))\n",
    "    # 1x1 convolution\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(25, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters = 25, kernel_size = (1,1), strides = (1,1), padding = 'valid', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2DTranspose(25, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(1, (1,1), padding=\"same\", activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def auto_encoder():\n",
    "    inputs=Input(train.shape[1:])\n",
    "    conv1 = Conv2D(30, (1, 1), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(20, (1, 1), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(30, (1, 1), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2DTranspose(40, (1, 1), padding=\"same\")(conv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(conv1)\n",
    "    model=Model(inputs=inputs, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def resnet_model2(start_neurons):\n",
    "\n",
    "    inputs=Input((40,40,13))\n",
    "    conv1 = Conv2D(start_neurons * 1, (2, 2), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    \n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(start_neurons * 1, (1, 1), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 1, (2, 2), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 4, (1, 1), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "\n",
    "\n",
    "    pool1 = Conv2D(start_neurons * 4, (1, 1), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "\n",
    "    add1 = add([conv2, pool1])\n",
    "\n",
    "\n",
    "    conv3 = Conv2D(start_neurons * 1, (1, 1), activation=\"relu\", padding=\"same\")(add1)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(start_neurons * 1, (2, 2), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "\n",
    "    conv3 = Conv2D(start_neurons * 4, (1, 1), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    add2 = add([conv3, add1])\n",
    "    \n",
    "    conv4 = Conv2D(start_neurons * 1, (1, 1), activation=\"relu\", padding=\"same\")(add2)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(start_neurons * 1, (2, 2), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(start_neurons * 4, (1, 1), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "\n",
    "    add3 = add([conv4, add2])\n",
    "\n",
    "\n",
    "    conv5 = Conv2D(start_neurons * 2, (1, 1), activation=\"relu\", padding=\"same\")(add3)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(start_neurons * 2, (2, 2), activation=\"relu\", padding=\"same\")(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "\n",
    "    conv5 = Conv2D(start_neurons * 8, (1, 1), activation=\"relu\", padding=\"same\")(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    \n",
    "    conv6 = Conv2D(start_neurons * 8, (1, 1), activation=\"relu\", padding=\"same\")(add3)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "\n",
    "    add4 = add([conv5, conv6])\n",
    "    \n",
    "    deconv = Conv2DTranspose(start_neurons * 2, (1, 1), padding=\"same\")(add4)\n",
    "    conv7 = Conv2D(start_neurons * 4, (1, 1), activation=\"relu\", padding=\"same\")(deconv)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(start_neurons * 2, (1, 1), activation=\"relu\", padding=\"same\")(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(start_neurons * 1, (1, 1), activation=\"relu\", padding=\"same\")(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(conv7)\n",
    "    model=Model(inputs=inputs, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_initializer='he_normal'\n",
    "def resnet_model():\n",
    "    inputs=Input(train.shape[1:])\n",
    "    \n",
    "    bn=BatchNormalization()(inputs)\n",
    "    conv0=Conv2D(256, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    \n",
    "    bn=BatchNormalization()(conv0)\n",
    "    conv=Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat=concatenate([conv0, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    conv=Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat=concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    # 원래는 5였음 - 6분정도 걸리고 2로 줄이면 3분정도 걸림 /에폭당\n",
    "    for i in range(3):\n",
    "        bn=BatchNormalization()(concat)\n",
    "        conv=Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(bn)\n",
    "        concat=concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    outputs=Conv2D(1, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    \n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('epochs_8.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for train_idx in k_fold.split(train):\n",
    "    indexes.append(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 로드해서 잘 복원되는지 확인해볼것.\n",
    "# import pickle\n",
    "# with open('submissions/22_23_kfold_index.pickle', 'wb') as f:\n",
    "#     pickle.dump(indexes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "indexes[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold 모델 훈련 및 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "FOLD = 4\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "EPOCHS = 100\n",
    "history = []\n",
    "scores = []\n",
    "\n",
    "result = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_number = 0\n",
    "\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "    print(x_train.shape)\n",
    "\n",
    "#     U net\n",
    "#     model = Model(input_layer, output_layer)\n",
    "    \n",
    "#     resnet 모델 - 지금까지는 베스트\n",
    "    model = resnet_model()\n",
    "    \n",
    "#     resnet 모델2\n",
    "#     model = resnet_model2(13)\n",
    "    \n",
    "#     auto-encoder 모델\n",
    "#     model = auto_encoder()\n",
    "    \n",
    "#   1x1 convolution 모델\n",
    "#     model = deep_cnn_advanced_nin()\n",
    "    \n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    \n",
    "    es = EarlyStopping(patience=5, verbose=1)\n",
    "    mc = ModelCheckpoint(f'best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.8)\n",
    "    csv_logger = CSVLogger(f'training_{model_number}.csv')\n",
    "    \n",
    "    model_ = model.fit(x_train, y_train, epochs = EPOCHS, validation_data=(VAL_X, VAL_Y), verbose=1, batch_size = 64, callbacks = [es, mc, rlp, csv_logger])\n",
    "    \n",
    "    history.append(model_)\n",
    "    model.load_weights(f'best_{model_number}.h5')\n",
    "    result += model.predict(TEST) / FOLD\n",
    "    \n",
    "    pred = model.predict(test)\n",
    "    score = maeOverFscore(test_y, np.exp(pred)-1)\n",
    "    print('mae / fscore: ' + str(score) + '\\n')\n",
    "    scores.append(score)\n",
    "    print(scores)\n",
    "    pred = model.predict(VAL_X)\n",
    "    score = maeOverFscore(VAL_Y, np.exp(pred)-1)\n",
    "    print('VAL_X  mae / fscore: ' + str(score) + '\\n')\n",
    "    model_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(history[0].history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 리더보드와 비슷한 테스트셋 찾는 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model = resnet_model()\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "sample_score = 0\n",
    "\n",
    "while sample_score < 1.48:\n",
    "    sample = random.sample(range(30662),2416)\n",
    "    sample_x = train[sample]\n",
    "    sample_y = train_y[sample]\n",
    "    \n",
    "    tmp = 0\n",
    "    \n",
    "    for i in range(2):\n",
    "        model.load_weights(f'best_{i}.h5')\n",
    "        res = model.predict(sample_x)\n",
    "        tmp += (np.exp(res)-1) / 2\n",
    "        \n",
    "    sample_score = maeOverFscore(sample_y, tmp)\n",
    "    print(sample_score)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inputs/val_index.pickle', 'wb') as f:\n",
    "    pickle.dump(sample, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inputs/val_x.pickle', 'wb') as f:\n",
    "    pickle.dump(train[sample], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inputs/val_y.pickle', 'wb') as f:\n",
    "    pickle.dump(train_y[sample], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자체 test셋으로 검증\n",
    "리더보드와 매우 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('inputs/my_test_set.pickle', 'rb') as f:\n",
    "#     test, test_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = 0\n",
    "tmp = 0\n",
    "for i in range(4):\n",
    "#     model = Model(input_layer, output_layer)\n",
    "#     model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    model.load_weights(f'best_{i}.h5')\n",
    "    res = model.predict(VAL_X)\n",
    "    tmp += (np.exp(res)-1) / 4\n",
    "    \n",
    "model_score = maeOverFscore(VAL_Y, tmp)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_score = 0\n",
    "tmp = 0\n",
    "for i in range(4):\n",
    "#     model = Model(input_layer, output_layer)\n",
    "#     model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    model.load_weights(f'best_{i}.h5')\n",
    "    res = model.predict(test)\n",
    "    tmp += (np.exp(res)-1) / 4\n",
    "    \n",
    "model_score = maeOverFscore(test_y, tmp)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST로 submission 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in range(4):\n",
    "#     model = Model(input_layer, output_layer)\n",
    "#     model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    model.load_weights(f'best_{i}.h5')\n",
    "    res = model.predict(TEST)\n",
    "    result += (np.exp(res)-1) / 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('kfold_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp32 = pd.read_csv('submissions/32_kfold_resnet2_log2.csv')\n",
    "tmp34 = pd.read_csv('submissions/34_kfold_unet.csv')\n",
    "\n",
    "tmp35 = tmp32.copy()\n",
    "tmp35.iloc[:, 1:] = 0.8 * tmp32.iloc[:, 1:] + 0.2 * tmp34.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp35.to_csv('tmp36.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tmp1 = pd.read_csv('22_unet_1.csv')\n",
    "tmp2 = pd.read_csv('23_kfold_resnet_4.csv')\n",
    "\n",
    "tmp1.iloc[:, 1:] = (tmp1.iloc[:, 1:] * 0.5) + (tmp2.iloc[:, 1:] * 0.5)\n",
    "\n",
    "tmp1.to_csv('ensemble_22_23.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해야할 일\n",
    "\n",
    "1. ~~스케일링(Scaling) - Robust, log, Standard Scaler  ~~  \n",
    "스케일링 방법을 다르게 해야하나. 하나의 사진을 기준으로 처리해야 할지 아니면 1번픽셀, 2번픽셀... 1600픽셀 끼리 해야하나\n",
    "2. ~~위도 경도 피쳐를 가지고 위도+경도, 위도-경도 피쳐 만들기(45도 회전변환)~~: 30, 60도 변환 추가\n",
    "3. 현재 custom metric이 작동하지 않으므로, 이 또한 수정해야함.\n",
    "4. tree모델 만들고, 앙상블\n",
    "5. ~~target 값에 log 취하기~~: 점수 안오름\n",
    "6. 순서가 바뀌긴 했으나, EDA 작업하자.\n",
    "7. 강수량 타일이 10개 미만인 것의 평균, 20개미만의 평균.... 50개 미만의 평균... 알아보자\n",
    "8. ~~augumentation 사용해서 강수량이 일정수준 이상인 사진 augementation 하자~~ - 다만, regression인 만큼 가장자리 어떻게 처리할지 생각해보자. - 정사각형 사진이기에 90도 돌린사진, 180도 돌린사진, 270도 돌린사진 이렇게만 해도 데이터가 증식될거같음 -  \n",
    "**그런데!!!!!** 생각해봐야할것이.....  \n",
    "위도, 경도 등 여러 요소가 존재할텐데 돌려버리면 원래는 비 안오는거를 비온다고 볼 수도 있을 듯 싶다. 쉽게 예를 들면 I 모양일때는 비가 오고, ㅡ 모양일때는 비가안오는데, 돌리는 바람에 안오는걸 온다고 할 수도 있을 것 같다. - 이건 테스트해볼수밖에 없을 듯 싶다.  \n",
    "돌릴수있는방법도 여러가지이다. 90도씩회전하기, 수직축기준으로 전환, 수평축기준으로 전환 등 - 캐글에서는 수직축, 수평축으로 전환해서 augmentation 썼다고 함.  위성이 도는 방향을 보면, 270도 돌리는 것도 괜찮을듯\n",
    "9. pretrained 모델 활용방안 생각하기 - 최소 사진크기가 존재하는데 우리는 40x40이라 작은편.. 이걸 40x40을 이어 붙여서 200x200으로 만들어서 넣으면 될거같은데 시도해보자.\n",
    "10. 9번채널 카테고리 변수 원핫인코딩\n",
    "11. Unet 모델과 FPN 모델 앙상블해서 제출하자\n",
    "12. loss='mse', metric=['mse', 'mae'] 이렇게 해보자\n",
    "13. U_res_net 구현하기\n",
    "14. 회전변환한 걸로 U_net 돌려서 제출하기\n",
    "15. off nadir viewing angle, earth incidence angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 리더보드(5부터) | mae / fscore | mae | 비고 |\n",
    "|---|:---:|---:|---:|\n",
    "| 1.93737 | 1.8744 | 0.2451 |  |\n",
    "| 1.75177 | 1.7016 | 0.2384 |  |\n",
    "| 1.61793 | 1.4234 | 0.0832 | 리더보드랑 metric 점수랑 차이 많이 남. 해결해야함. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
