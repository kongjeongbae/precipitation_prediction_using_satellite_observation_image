submission 정리
1. RAINS = 50, FEATURES = 9, epochs = 10, loss="mae" - 3.1437019608
2. RAINS = 50, FEATURES = 14, epochs = 10, loss="mae" - 2.4486110373
3. RAINS = 30, FEATURES = 14, epochs = 10, loss="mae" - 2.6535480502
4. RAINS = 0, FEATURES = 14, epochs = 10, loss="mae", RobustScaler - 1.9926990874	
5. RAINS = 0, FEATURES = 14, epochs = 10, loss="mae", StandardScaler - 1.9373728258	

6. RAINS = 0, FEATURES = 18, epochs = 30, loss="mae", StandardScaler - 1.75177 (아웃라이어 들어있던 데이터셋)
경도위도피쳐1 과 경도위도피쳐2를 45도 기울인 피쳐 생성 (14개 피쳐 -> 18개 피쳐)
[2.603283393857084, 2.1306794636920117, 1.9457942271557496, 1.9418761465190595, 1.9211344964102526,
 2.0774277030172996, 1.8636660398033436, 1.8054401698943567, 1.8447519738116804, 1.8598753563348698,
 1.8243144879657673, 1.7833311937372518, 1.7789104792586061, 1.7933836181810932, 1.7795586498856169,
 1.795911002596416, 1.7460888641762113, 1.72916131775662, 1.706605378891823, 1.7151047134320048,
 1.723709926870694,  1.6753284020042793, 1.7622319722637765, 1.6598363217736527, 1.6842478909727712,
 1.7136034857819324,  1.6619449837167755, 1.6975781363773166, 1.6479536718597123, 1.7015916401624267]
 
제출안함: 45도 회전변환
epochs: 30, 60, 90, 120 : [1.7025365718942789, 1.5875933156644357, 1.590171123467864, 1.6228802266170252]


7. RAINS = 0, FEATURES = 26, epochs = 300, loss="mae", StandardScaler - 1.61793 - 에폭 많이 늘리고, 피쳐도 많이 늘림
아웃라이어(-9999) 완벽히 제거한 테스트셋 45도, 30도, 60도 회전변환
epochs: 30, 60, 90, 120 .... : [1.7091541549842841, 1.6004492446471008, 1.596174260882447, 1.5203467328624916, 1.5505140792745995,
                                1.4979194776845146, 1.4992410371027813, 1.4795495791831075, 1.5207863146099385, 1.4234108652649777]
                                

8. RAINS = 0, FEATURES = 26, epochs = 300, loss="mae", StandardScaler - 1.61793 - 에폭 많이 늘리고, 피쳐도 많이 늘림
 테스트셋 45도, 30도, 60도 회전변환
train_y 에 log를 씌웠음. 0이 많고, 아웃라이어가 있기때문에
하지만 점수 나빠짐 - 2.1467044302                            
epochs: 30, 60, 90, 120 .... : [1.7296326914926763, 1.6605600481461622, 1.6436478921389022, 1.604661953992397, 1.5379761051767218,
                                1.553698927766043, 1.5742461622277115, 1.5286148414489071, 1.5129169827053468, 1.4997900214824922]

9. 7과 같고, 에폭을 150
리더보드 - 1.7158514602
메트릭 - 1.581789

10. 지표channel 삭제, 에폭 320
리더보드 - 1.7031844648
메트릭 - 1.5448043911286744

11. 지표channel 넣었고, 90도 회전한 사진 추가함(augmentation).  에폭 210
리더보드 - 1.7545135622
메트릭 - 1.6644586
[1.700433449954057, 1.6710699496137547, 1.6631121669538391, 1.6768228129930394, 1.6129802429869702, 
1.6130112619634103, 1.6644586375284691

12.
conv2d 층을 하나 더 집어 넣었고, 에폭을 450번 돌림. 리더보드와 차이가 너무 심함. 과적합인거같아.
리더보드 - 1.6859151798
cv- 1.384855

13. 위와 같고 에폭만 600으로.
리더보드 - 1.6447785653
cv - 1.3025737741

14. catboost 모델
리더보드 - 4.7880249802
cv - 2.58902612800947

15,16 - 180도 로테이션한 파일 더해서 모델 돌림 에폭 300, 330
[1.7503484504187141, 1.6202108034052416, 1.5866156803133102, 1.5631115648340084, 1.5586615610698646, 1.563160728361566,
 1.5251933047432003, 1.5188241161569431, 1.493142462699831, 1.5042745760774714, 1.4800554084281783, 1.482323648176912]
 
 
 
rot480 모델
[1.7503484504187141, 1.6202108034052416, 1.5866156803133102, 1.5631115648340084, 1.5586615610698646, 1.563160728361566,
 1.5251933047432003, 1.5188241161569431, 1.493142462699831, 1.5042745760774714, 1.4800554084281783, 1.482323648176912,
 1.4794896710310008, 1.4871196067034211, 1.4884443454946035, 1.461928590818076]
 
22.
K-FOLD + Unet 모델 - 5/15-1

train1 = np.rot90(train, 1, (1,2))
train2 = np.rot90(train, 2, (1,2))
train3 = np.rot90(train, 3, (1,2))
train_lr = np.fliplr(train)
up는 메모리 부족때문에 못함.

리더보드 - 1.6821320439
cv - 1.672146381677924
val_loss - 0.2124
에폭 300 - es 약 15
fold 4
patience=5

23.
K-FOLD + Resnet 모델 - 5/15-2

train1 = np.rot90(train, 1, (1,2))
train2 = np.rot90(train, 2, (1,2))
train3 = np.rot90(train, 3, (1,2))
train_lr = np.fliplr(train)
up는 메모리 부족때문에 못함.

리더보드 - 1.5913076614
cv - 1.6009544040
val_loss
[0.22564828229964912, 0.23061252887519504, 0.2226697347541847, 0.21834500440467894, 0.2151899141904215, 0.21399040412210607, 0.2133958670738581, 0.21103569689857982, 0.2105252258481589, 0.214065571636129, 0.21017461255619416, 0.21551606530392906, 0.20879970257346694, 0.20897569276047545, 0.20951805884885316, 0.2078590255522747, 0.20988936084842247, 0.20639468737789932, 0.2052247216437059, 0.20800634425433523, 0.2035913207567979, 0.20419117748689486, 0.20831461263063408, 0.20980090317175545, 0.204694755969119, 0.20399748393007228]
[0.2302055599441042, 0.23768600823488842, 0.21996688537564607, 0.22076771598506764, 0.21997011071681377, 0.21453340042584548, 0.21380692094125006, 0.21237230282081976, 0.21252827668879692, 0.21384825990460926, 0.21169012397924059, 0.20946482564094487, 0.21135706955175174, 0.20916883527296293, 0.207214582232451, 0.20779332423274582, 0.20719052879791003, 0.21136799545924695, 0.20802131922608882, 0.20660607610991943, 0.20582888910186573, 0.2060094019216908, 0.2099816460204171, 0.20658194175715647, 0.20622534432758655, 0.20729099905801268]
[0.22614116040211832, 0.22449701908700598, 0.22453903604223366, 0.21720644886287158, 0.2117146153968269, 0.21345517136276784, 0.2095156982842146, 0.20922717407308689, 0.20984275415115064, 0.21857442599976332, 0.20857909875020839, 0.21217850698930987, 0.2442401870927658, 0.21096724473665107, 0.20884164935612162, 0.20640110206082748, 0.20294013304484404, 0.2033303258435402, 0.20457459411201856, 0.20331602400817406, 0.20554444574493735, 0.20611583269412187]
[0.22703363430967693, 0.21856524968773167, 0.22228809837935273, 0.21722375337532812, 0.21509492627869814, 0.21266352689195386, 0.21385105773449684, 0.20997557557628627, 0.21226533044028625, 0.21034258842426792, 0.22005679602670072, 0.21062121793578692, 0.2095507692766265, 0.21381061734477289, 0.2112183186033118, 0.21117530863277217, 0.22374824009543232, 0.21829763957993104]
에폭 300 - es 약 15
fold 4
patience=5

모델은 저장되어 있으니 

24. 앙상블 22 + 23 - 5/15-3
리더보드 - 1.6047097366
23에 더 가중치를 주면 점수 개선 여지 있음
0.8 0.2 이 정도면 될듯

25.
K-FOLD + Resnet 모델 - 5/16-1
데이터셋은 회전변환한거, 1번채널, 3번채널 삭제(0, 2번과 상관관계가 너무 높아서)

train1 = np.rot90(train, 1, (1,2))
train2 = np.rot90(train, 2, (1,2))
train3 = np.rot90(train, 3, (1,2))
train_lr = np.fliplr(train)
up는 메모리 부족때문에 못함.

중간에 TEST 셋을 잘못지정해서 fold 1 모델만 가지고 제출했더니
fold 1만 가지고 제출했더니 리더보드와 cv 값이 차이가 많이 났다.
이게 fold 1만 해서 그런지
아니면 데이터셋이 달라져서 그런지는 아직 모르겠음.

리더보드 - 1.6038627883	
cv - 1.5383828275127052
val_loss
[0.22881625951580545, 0.2251683336968469, 0.22003266653865935, 0.2211240375228575, 0.21795775035517337, 0.21659237870979922,
 0.21536267780590926, 0.21393602427024397, 0.2135651221218229, 0.21339630817534422, 0.21064717633809704, 0.21233672453800817,
 0.21212906172274268, 0.2146799546561523, 0.20839212103533752, 0.20972764716688758, 0.2098290347641472, 0.20922304440189052,
 0.20757262266620072, 0.20733745314792926, 0.20674350516567072, 0.2078145519429297,0.20665445504802288, 0.20668822469910997,
 0.20767285956684395, 0.20815787881588854, 0.20670761115368527, 0.20681405940698946, 0.20534326900589234, 0.20553445027319225,
 0.2057104862283378, 0.20598304898049188, 0.205932325372761, 0.20638370676843884, 0.20536588861768756, 0.20584664275804432,
 0.20551091073619704, 0.20689447855356066, 0.20483296623611502, 0.20531930002915466, 0.2054626520070157, 0.20529123275937772,
 0.20487551249888672, 0.20523284587960272, 0.2049417469450137, 0.20532578707921623, 0.2050902195647658, 0.20519067557667678,
 0.20496143485043883]

에폭 50 - 
fold 1
es = EarlyStopping(patience=10, verbose=1)
rlp = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.8)


26.
리더보드 - 1.58212
cv - 1.517914227919983

앞의 fold1에 더해서 2,3,4 추가해서 submission 제작
근데 문제가 25번도 리더보드와 cv간 약 0.065정도 차이나고
26도 차이가 난다.

어제까지는 거의 차이가 없었는데 이게 문제인듯
내 생각엔 지금 축 2개를 빼서 오류가 더 심해진거같은데
테스트 해봐야겠다.

27.
리더보드 - 4.1939856189	
cv - 1.5208587049955082

26번과 동일하고, 단지 1, 3번 채널 삭제 안하고 돌렸는데 리더보드 점수가 너무 이상하게 나옴.


28. 29.

27번이 이상하게 나온 이유를 알아냄. 28.29번도 마찬가지인데 회전변환할때 TEST 를 이상하게 회전시켜버림


30.
리더보드 - 1.54086
cv - 1.4790608

약 0.06정도 차이 남
위와는 달리 TEST 정확하게 회전시킴.
26번과 달리 채널 2개를 더 넣어서 모델 만들었는데 리더보드와 cv 간 차이가 거의 비슷한거보니, 이 문제는 아닌거 같다.

fold는 한번만 진행했음.
타겟값에 로그를 취함.

31. resnet2 - 원래 resnet은 pooling이 있는데 나는 안했음
9번채널 322로 나눴음.  5,6채널 회전변환 30도에서 45도로 바꿈.
리더보드 -1.7762737777
cv -1.224236860801549
굉장히 차이가 많이남.


32. resnet
9번채널 322로 나눴음.  5,6채널 회전변환 30도에서 45도로 바꿈.
리더보드 - 1.5143479822
cv -1.2504409752725603
굉장히 차이가 많이남. 왜이렇게 차이가 많이 나는지생각해보자.

32-1. resnet - 제출안함
원핫인코딩 적용
cv- 1.21916223139587

33. U-net
오랜만에 Unet 돌림. 1에폭에 1분정도 걸림. resnet은 5분30초정도 걸리는데 비해 매우 빨리 돌아감.

리더보드 - 1.6468641381
cv - 1.2795952725892008
[1.2488897075714978, 1.2628742622832885, 1.4038622072680864, 1.3020768829230611]

이건 차이가 더 심함....


34 - resnet
비가 많이 내린 사진을 엄청 부풀렸음. 하지만 사진 회전 augmentation은 안해서 데이터셋이 32번보다는 엄청 적었음
레스넷 깊이도 루프문 5개에서 루프문 3으로 줄였음.
모델 돌아가는 시간 거의 20%수준으로 낮아졌음.

리더보드 - 1.5731414649
아웃라이어 많은 cv - 1.3468869990138457
Kfold시 val - 2.1325424012796184

아무래도 비가많이 내린 사진에 집중한 모델이다 보니 아웃라이어 많은 cv에서 효과적임을 보였다.


35 - 앙상블
32*0.6 + 34*0.4
리더보드 - 1.5162884129	

36 - 앙상블
32*0.8 + 34*0.2
리더보드 - 1.5116624356
매우조금 개선되긴했는데 의미가 없어보임.
그냥 모델 정확도를 올리는게 더 먼저일듯.

============== 이제 제출 18번밖에 안남았다. =================

37.
리더보드 -	1.5656786257
cv - [1.5399455859607607, 1.483895422948927, 1.476838919621813, 1.5461670318394143]
[1.2820981482652314, 1.2525514321427658, 1.2630321999207776, 1.288787697255058]

34에 비해 데이터양을 줄임 다른건 같음
train = np.vstack([train, up50])
train = np.vstack([train, up100]) 이것만 함
너무 안맞음 cv와 리더보드가


########
현재 U_res_net 만들었고 테스트 해볼 예정

 