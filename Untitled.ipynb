{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def mae_over_fscore(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: sample_submission.csv 형태의 실제 값\n",
    "    y_pred: sample_submission.csv 형태의 예측 값\n",
    "    '''\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(1, -1)[0]  \n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
    "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
    "    \n",
    "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
    "    IsNotMissing = y_true >= 0\n",
    "    \n",
    "    # mae 계산\n",
    "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
    "    \n",
    "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
    "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    # f1_score 계산    \n",
    "    f_score = f1_score(y_true, y_pred) \n",
    "    \n",
    "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
    "    return mae / (f_score + 1e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# metric 정의\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "# 이거 쓰면 됨.\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    \n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "def maeOverFscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_over_fscore(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: sample_submission.csv 형태의 실제 값\n",
    "    y_pred: sample_submission.csv 형태의 예측 값\n",
    "    '''\n",
    "\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(1, -1)[0]  \n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
    "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
    "    \n",
    "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
    "    IsNotMissing = y_true >= 0\n",
    "    \n",
    "    # mae 계산\n",
    "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
    "    \n",
    "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
    "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    # f1_score 계산    \n",
    "    f_score = f1_score(y_true, y_pred) \n",
    "    \n",
    "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
    "    return mae / (f_score + 1e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "with open('D:/inputs/24/train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('D:/inputs/24/train_y.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "with open('D:/inputs/24/test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "train_y = train_y.reshape(75971, 40, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "train.shape,\n",
    "train_y.shape,\n",
    "test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(75971, 40 * 40 * 14)\n",
    "test = test.reshape(2416, 40 * 40 * 14)\n",
    "\n",
    "LEN_TRAIN = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_np = np.vstack([train, test])\n",
    "\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reshape(75971, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "ss = StandardScaler()\n",
    "all_np = ss.fit_transform(all_np)\n",
    "\n",
    "# rs = RobustScaler()\n",
    "# all_np = rs.fit_transform(all_np)\n",
    "\n",
    "train = all_np[:LEN_TRAIN]\n",
    "test = all_np[LEN_TRAIN:]\n",
    "\n",
    "del all_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(75971, 40, 40, 14)\n",
    "test = test.reshape(2416, 40, 40, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해야할 일\n",
    "\n",
    "1. 스케일링(Scaling) - Robust, log, Standard Scaler\n",
    "2. 위도 경도 피쳐를 가지고 위도+경도, 위도-경도 피쳐 만들기\n",
    "3. 현재 custom metric이 작동하지 않으므로, 이 또한 수정해야함.\n",
    "4. tree모델 만들고, 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def build_model(input_layer, start_neurons):\n",
    "    \n",
    "    # 40 x 40 -> 20 x 20\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 20 x 20 -> 10 x 10\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "    pool2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    # 10 x 10 \n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    # 10 x 10 -> 20 x 20\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.25)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    # 20 x 20 -> 40 x 40\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((40, 40, 14))\n",
    "output_layer = build_model(input_layer, 32)\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras])\n",
    "model.compile(loss=loss, optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "scores = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 리더보드 | mae / fscore | mae |\n",
    "|---|:---:|---:|\n",
    "| 1.93737 | 1.8744 | 0.2451 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predicted_y, desired_y):\n",
    "    predictions = tf.argmax(desired_y, axis=-1)\n",
    "    pr, pr_op = tf.keras.metrics.Precision(predicted_y, predictions)\n",
    "    re, re_op = tf.keras.metrics.Recall(predicted_y, predictions)\n",
    "    f1 = (2 * pr * re) / (pr + re)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - desired_y)) / f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model_history = model.fit(train, train_y, epochs = 1, verbose=1, batch_size = 200)\n",
    "    score = maeOverFscore(train_y, model.predict(train))\n",
    "    print('mae / fscore: ' + str(score) + '\\n')\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)\n",
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "submission.iloc[:,1:] = pred.reshape(-1, 1600)\n",
    "submission.to_csv('Dacon_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission 정리\n",
    "1. RAINS = 50, FEATURES = 9, epochs = 10, loss=\"mae\" - 3.1437019608\n",
    "2. RAINS = 50, FEATURES = 14, epochs = 10, loss=\"mae\" - 2.4486110373\n",
    "3. RAINS = 30, FEATURES = 14, epochs = 10, loss=\"mae\" - 2.6535480502\n",
    "4. RAINS = 0, FEATURES = 14, epochs = 5, loss=\"mae\", RobustScaler - 1.9926990874\t\n",
    "5. RAINS = 0, FEATURES = 14, epochs = 5, loss=\"mae\", StandardScaler - 1.9373728258\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2,2), padding=\"same\",input_shape=(40,40,14)))\n",
    "model.add(Conv2D(1, (1,1), padding=\"same\", activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras]) # custom metric으로 하면 실행이 안됨\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train, train_y, batch_size = 100, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(test2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
