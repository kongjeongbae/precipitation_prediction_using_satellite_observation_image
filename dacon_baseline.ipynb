{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "import warnings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# 재생산성을 위해 시드 고정\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 피쳐 개수 지정\n",
    "FEATURES = 14\n",
    "\n",
    "# 40*40 픽셀에서 몇 픽셀이상에 비가 내려야 데이터로 사용할 지 정하는 기준 값\n",
    "RAINS = 50\n",
    "\n",
    "def trainGenerator():\n",
    "    \n",
    "    train_path = 'inputs/train'\n",
    "    train_files = sorted(glob.glob(train_path + '/*'))\n",
    "    \n",
    "    for file in train_files:\n",
    "        \n",
    "        dataset = np.load(file)\n",
    "        \n",
    "        target= dataset[:,:,-1].reshape(40,40,1)\n",
    "        \n",
    "        cutoff_labels = np.where(target < 0, 0, target) \n",
    "        outliers = np.where(target < 0, target, 0)\n",
    "        # np.where(조건, 조건에 맞을 때 값, 조건과 다를 때 값)\n",
    "        # x = np.array([5, 4, 3, 2, 1, 0])\n",
    "        # np.where(x >= 3, 3, x)\n",
    "        # array([3, 3, 3, 2, 1, 0])\n",
    "        \n",
    "        feature = dataset[:,:,:FEATURES]\n",
    "        \n",
    "        # 40*40 1600픽셀 중 비내린 픽셀이 RAINS개 미만이면 찍혀있으면 패스. 최소 30개 이상의 픽셀에 비가 내려야지 fit에 사용.\n",
    "        if (cutoff_labels > 0).sum() < RAINS:\n",
    "            continue\n",
    "            \n",
    "        # 아웃라이어(-9999)가 존재하면 패스\n",
    "        if (outliers < 0).sum() < 0:\n",
    "            continue\n",
    "\n",
    "        yield (feature, cutoff_labels)\n",
    "        \n",
    "train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([40,40,FEATURES]),tf.TensorShape([40,40,1])))\n",
    "train_dataset = train_dataset.batch(512).prefetch(1)\n",
    "test_path = 'inputs/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for file in tqdm(test_files, desc = 'test'):\n",
    "    \n",
    "    data = np.load(file)\n",
    "    \n",
    "    X_test.append(data[:,:,:FEATURES])\n",
    "                  \n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "color_map = plt.cm.get_cmap('RdBu')\n",
    "color_map = color_map.reversed()\n",
    "image_sample = np.load('inputs/train/subset_010462_02.npy')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.imshow(image_sample[:, :, i], cmap=color_map)\n",
    "\n",
    "plt.subplot(1,20,20)\n",
    "plt.imshow(image_sample[:,:,-1], cmap = color_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_layer, start_neurons):\n",
    "    \n",
    "    # 40 x 40 -> 20 x 20\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 20 x 20 -> 10 x 10\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "    pool2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    # 10 x 10 \n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    # 10 x 10 -> 20 x 20\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.25)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    # 20 x 20 -> 40 x 40\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((40, 40, FEATURES))\n",
    "output_layer = build_model(input_layer, 32)\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    \n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "def maeOverFscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras])\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history = model.fit(train_dataset, epochs = 10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "submission.iloc[:,1:] = pred.reshape(-1, 1600)\n",
    "submission.to_csv('Dacon_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission 정리\n",
    "1. RAINS = 50, FEATURES = 9, epochs = 10, loss=\"mae\" - 3.1437019608\n",
    "2. RAINS = 50, FEATURES = 14, epochs = 10, loss=\"mae\" - 2.4486110373\n",
    "3. RAINS = 30, FEATURES = 14, epochs = 10, loss=\"mae\" - 2.6535480502"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해야할 일\n",
    "- scaling 처리 - robust, standard, log\n",
    "- pretrained 모델 활용\n",
    "- 위도 경도 피쳐를 가지고 위도+경도, 위도-경도 피쳐 만들기\n",
    "- 현재 custom metric이 작동하지 않으므로, 이 또한 수정해야함.\n",
    "- tree모델 만들고, 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
