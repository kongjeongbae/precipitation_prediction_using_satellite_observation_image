{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 및 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from custom_metric import *\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "with open('D:/inputs/24/train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('D:/inputs/24/train_y.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "    train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "\n",
    "with open('D:/inputs/24/test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "LEN_TRAIN = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "163.022018 - 161.256897 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    train.shape,\n",
    "    train_y.shape,\n",
    "    test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건에 맞는 사진 개수 세는 알고리즘.\n",
    "# from tqdm import tqdm\n",
    "# cnt = 0\n",
    "# for i in tqdm(train_y):\n",
    "#     for ii in i:\n",
    "#         if (ii == 0).sum() != 40:\n",
    "#             cnt += 1\n",
    "# cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9번 channel 가공 - 카테고리형 피쳐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = np.delete(train, 9, axis=3)\n",
    "# test = np.delete(test, 9, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/inputs/24/rot_train.pickle', 'rb') as f:\n",
    "    rot_train = pickle.load(f)\n",
    "with open('D:/inputs/24/rot_train_y.pickle', 'rb') as f:\n",
    "    rot_train_y = pickle.load(f)\n",
    "    rot_train_y = rot_train_y.reshape(rot_train_y.shape[0], 40, 40, 1)\n",
    "LEN_TRAIN = train.shape[0] + rot_train.shape[0]\n",
    "\n",
    "train = np.vstack([train, rot_train])\n",
    "train_y = np.vstack([train_y, rot_train_y])\n",
    "\n",
    "del rot_train\n",
    "del rot_train_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all_np로 결합 후 scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_np = np.vstack([train, test])\n",
    "del train, test\n",
    "\n",
    "rotation_45_1_1 = all_np[:, :, :, -4] + all_np[:, :, :, -3]\n",
    "rotation_45_1_2 = all_np[:, :, :, -4] - all_np[:, :, :, -3]\n",
    "rotation_45_2_1 = all_np[:, :, :, -2] + all_np[:, :, :, -1]\n",
    "rotation_45_2_2 = all_np[:, :, :, -2] - all_np[:, :, :, -1]\n",
    "\n",
    "rotation_45_1_1 = rotation_45_1_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_45_1_2 = rotation_45_1_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_45_2_1 = rotation_45_2_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_45_2_2 = rotation_45_2_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "\n",
    "all_np = np.concatenate((all_np, rotation_45_1_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_45_1_2), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_45_2_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_45_2_2), axis=-1)\n",
    "\n",
    "del rotation_45_1_1\n",
    "del rotation_45_1_2\n",
    "del rotation_45_2_1\n",
    "del rotation_45_2_2\n",
    "\n",
    "\n",
    "rotation_30_1_1 = all_np[:, :, :, -4] * np.cos(np.pi / 6) + all_np[:, :, :, -3] * np.sin(np.pi / 6)\n",
    "rotation_30_1_2 = all_np[:, :, :, -4] * np.cos(np.pi / 6) - all_np[:, :, :, -3] * np.sin(np.pi / 6)\n",
    "rotation_30_2_1 = all_np[:, :, :, -2] * np.cos(np.pi / 6) + all_np[:, :, :, -1] * np.sin(np.pi / 6)\n",
    "rotation_30_2_2 = all_np[:, :, :, -2] * np.cos(np.pi / 6) - all_np[:, :, :, -1] * np.sin(np.pi / 6)\n",
    "\n",
    "rotation_30_1_1 = rotation_30_1_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_30_1_2 = rotation_30_1_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_30_2_1 = rotation_30_2_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_30_2_2 = rotation_30_2_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "\n",
    "all_np = np.concatenate((all_np, rotation_30_1_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_30_1_2), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_30_2_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_30_2_2), axis=-1)\n",
    "\n",
    "del rotation_30_1_1\n",
    "del rotation_30_1_2\n",
    "del rotation_30_2_1\n",
    "del rotation_30_2_2\n",
    "\n",
    "\n",
    "rotation_60_1_1 = all_np[:, :, :, -4] * np.cos(np.pi / 3) + all_np[:, :, :, -3] * np.sin(np.pi / 3)\n",
    "rotation_60_1_2 = all_np[:, :, :, -4] * np.cos(np.pi / 3) - all_np[:, :, :, -3] * np.sin(np.pi / 3)\n",
    "rotation_60_2_1 = all_np[:, :, :, -2] * np.cos(np.pi / 3) + all_np[:, :, :, -1] * np.sin(np.pi / 3)\n",
    "rotation_60_2_2 = all_np[:, :, :, -2] * np.cos(np.pi / 3) - all_np[:, :, :, -1] * np.sin(np.pi / 3)\n",
    "\n",
    "rotation_60_1_1 = rotation_60_1_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_60_1_2 = rotation_60_1_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_60_2_1 = rotation_60_2_1.reshape(all_np.shape[0], 40, 40,1)\n",
    "rotation_60_2_2 = rotation_60_2_2.reshape(all_np.shape[0], 40, 40,1)\n",
    "\n",
    "all_np = np.concatenate((all_np, rotation_60_1_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_60_1_2), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_60_2_1), axis=-1)\n",
    "all_np = np.concatenate((all_np, rotation_60_2_2), axis=-1)\n",
    "\n",
    "del rotation_60_1_1\n",
    "del rotation_60_1_2\n",
    "del rotation_60_2_1\n",
    "del rotation_60_2_2\n",
    "\n",
    "\n",
    "train = all_np[:LEN_TRAIN]\n",
    "test = all_np[LEN_TRAIN:]\n",
    "del all_np\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "TR_SHAPE_0 = train.shape[0]\n",
    "TR_SHAPE_1 = train.shape[1]\n",
    "TR_SHAPE_2 = train.shape[2]\n",
    "TR_SHAPE_3 = train.shape[3]\n",
    "\n",
    "\n",
    "TE_SHAPE_0 = test.shape[0]\n",
    "TE_SHAPE_1 = test.shape[1]\n",
    "TE_SHAPE_2 = test.shape[2]\n",
    "TE_SHAPE_3 = test.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "train = train.reshape(TR_SHAPE_0 * TR_SHAPE_1 * TR_SHAPE_2, TR_SHAPE_3)\n",
    "test = test.reshape(TE_SHAPE_0 * TE_SHAPE_1 * TE_SHAPE_2, TE_SHAPE_3)\n",
    "all_np = np.vstack([train, test])\n",
    "\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# rs = RobustScaler()\n",
    "# mm = MinMaxScaler()\n",
    "# all_np = ss.fit_transform(all_np)\n",
    "\n",
    "for i in range(all_np.shape[1]):\n",
    "    ss = StandardScaler()\n",
    "    all_np[:,i]  = ss.fit_transform(all_np[:,i].reshape(-1, 1)).reshape(len(all_np))\n",
    "\n",
    "\n",
    "train = all_np[:TR_SHAPE_0 * TR_SHAPE_1 * TR_SHAPE_2]\n",
    "test = all_np[TR_SHAPE_0 * TR_SHAPE_1 * TR_SHAPE_2:]\n",
    "\n",
    "del all_np\n",
    "del ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(TR_SHAPE_0 , TR_SHAPE_1 , TR_SHAPE_2, TR_SHAPE_3)\n",
    "test = test.reshape(TE_SHAPE_0 , TE_SHAPE_1 , TE_SHAPE_2, TE_SHAPE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def build_model(input_layer, start_neurons):\n",
    "    \n",
    "    # 40 x 40 -> 20 x 20\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 20 x 20 -> 10 x 10\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "    pool2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    # 10 x 10 \n",
    "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "\n",
    "    # 10 x 10 -> 20 x 20\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.25)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "    # 20 x 20 -> 40 x 40\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = BatchNormalization()(uconv1)\n",
    "    uconv1 = Dropout(0.25)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation='relu')(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((40, 40, train.shape[3]))\n",
    "output_layer = build_model(input_layer, 32)\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras])\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "scores = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predicted_y, desired_y):\n",
    "    predictions = tf.argmax(desired_y, axis=-1)\n",
    "    pr, pr_op = tf.keras.metrics.Precision(predicted_y, predictions)\n",
    "    re, re_op = tf.keras.metrics.Recall(predicted_y, predictions)\n",
    "    f1 = (2 * pr * re) / (pr + re)\n",
    "    return tf.reduce_mean(tf.abs(predicted_y - desired_y)) / f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('epochs_8.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "for _ in range(5):\n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"epochs: {i+1}\")\n",
    "        model_history = model.fit(train, train_y, epochs = 1, verbose=1, batch_size = 200)\n",
    "    score = maeOverFscore(train_y, model.predict(train))\n",
    "    print('mae / fscore: ' + str(score) + '\\n')\n",
    "    scores.append(score)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "for _ in range(1):\n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"epochs: {i+1}\")\n",
    "        model_history = model.fit(train, train_y, epochs = 1, verbose=1, batch_size = 400)\n",
    "    score = maeOverFscore(train_y, model.predict(train))\n",
    "    print('mae / fscore: ' + str(score) + '\\n')\n",
    "    scores.append(score)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "for _ in range(1):\n",
    "    for i in range(1):\n",
    "        print(f\"epochs: {i+1}\")\n",
    "        model_history = model.fit(train, train_y, epochs = 1, verbose=1, batch_size = 400)\n",
    "    score = maeOverFscore(train_y, model.predict(train))\n",
    "    print('mae / fscore: ' + str(score) + '\\n')\n",
    "    scores.append(score)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('epochs_8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scores) * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)\n",
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "submission.iloc[:,1:] = pred.reshape(-1, 1600)\n",
    "submission.to_csv('Dacon_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**해야할 일**\n",
    "\n",
    "1. 스케일링(Scaling) - Robust, log, Standard Scaler  \n",
    "2. ~~위도 경도 피쳐를 가지고 위도+경도, 위도-경도 피쳐 만들기(45도 회전변환)~~: 30, 60도 변환 추가\n",
    "3. 현재 custom metric이 작동하지 않으므로, 이 또한 수정해야함.\n",
    "4. tree모델 만들고, 앙상블\n",
    "5. ~~target 값에 log 취하기~~: 점수 안오름\n",
    "6. 순서가 바뀌긴 했으나, EDA 작업하자.\n",
    "7. 강수량 타일이 10개 미만인 것의 평균, 20개미만의 평균.... 50개 미만의 평균... 알아보자\n",
    "8. ~~augumentation 사용해서 강수량이 일정수준 이상인 사진 augementation 하자~~ - 다만, regression인 만큼 가장자리 어떻게 처리할지 생각해보자. - 정사각형 사진이기에 90도 돌린사진, 180도 돌린사진, 270도 돌린사진 이렇게만 해도 데이터가 증식될거같음 -  \n",
    "**그런데!!!!!** 생각해봐야할것이.....  \n",
    "위도, 경도 등 여러 요소가 존재할텐데 돌려버리면 원래는 비 안오는거를 비온다고 볼 수도 있을 듯 싶다. 쉽게 예를 들면 I 모양일때는 비가 오고, ㅡ 모양일때는 비가안오는데, 돌리는 바람에 안오는걸 온다고 할 수도 있을 것 같다. - 이건 테스트해볼수밖에 없을 듯 싶다. \n",
    "9. pretrained 모델 활용방안 생각하기 - 최소 사진크기가 존재하는데 우리는 40x40이라 작은편.. 이걸 40x40을 이어 붙여서 200x200으로 만들어서 넣으면 될거같은데 시도해보자.\n",
    "10. 카테고리형 변수 원핫인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 리더보드(5부터) | mae / fscore | mae | 비고 |\n",
    "|---|:---:|---:|---:|\n",
    "| 1.93737 | 1.8744 | 0.2451 |  |\n",
    "| 1.75177 | 1.7016 | 0.2384 |  |\n",
    "| 1.61793 | 1.4234 | 0.0832 | 리더보드랑 metric 점수랑 차이 많이 남. 해결해야함. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
